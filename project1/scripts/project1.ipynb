{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_helper import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process\n",
    "\n",
    "num_feature = tX.shape[1]\n",
    "col_avg = np.zeros(num_feature)\n",
    "col_std = np.zeros(num_feature)\n",
    "\n",
    "for i in range(num_feature):\n",
    "    cur_col = tX[:, i]\n",
    "    if i == num_feature-1:\n",
    "        good_data = np.where(cur_col!=0)\n",
    "    else:\n",
    "        good_data = np.where(cur_col!=-999)\n",
    "    col_avg[i] = np.mean(cur_col[good_data])\n",
    "    col_std[i] = np.std(cur_col[good_data])\n",
    "    \n",
    "col_avg = col_avg[np.newaxis, :]\n",
    "col_std = col_std[np.newaxis, :]\n",
    "\n",
    "\n",
    "bad_data = np.where(tX==-999)\n",
    "bad_col_data = np.where(tX[:, -1]==0)\n",
    "\n",
    "tX = (tX-col_avg) / col_std\n",
    "tX[bad_data] = 0\n",
    "tX[:, -1][bad_col_data] = 0\n",
    "\n",
    "# std = np.std(tX, axis=0)\n",
    "# tX = tX / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required implementations\n",
    "\n",
    "# max_iters = 300\n",
    "# least square\n",
    "# gamma = 0.0000001\n",
    "# logistic\n",
    "# gamma = 1\n",
    "# batch_size = 32\n",
    "# lambda_ = 0.1\n",
    "# initial_w = np.random.rand(tX.shape[1]) * 2 - 1\n",
    "# initial_w = np.zeros(tX.shape[1])\n",
    "# weights, loss = least_squares_GD(y, tX, initial_w, max_iters, gamma)\n",
    "# weights, loss = least_squares_SGD(y, tX, initial_w, batch_size, max_iters, gamma)\n",
    "# weights, loss = least_squares(y, tX)\n",
    "# weights, loss = ridge_regression(y, tX, lambda_)\n",
    "# weights, loss = logistic_regression_GD(y, tX, initial_w, max_iters, gamma)\n",
    "# weights, loss = reg_logistic_regression_SGD(y, tX, lambda_, initial_w, batch_size, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial fit\n",
    "\n",
    "ratio = 0.9\n",
    "degree = 4\n",
    "seed = 6\n",
    "\n",
    "initial_w = np.zeros(tX.shape[1]*degree)\n",
    "\n",
    "x_train, y_train, x_val, y_val = split_data(tX, y, ratio, seed)\n",
    "x_train_poly = build_poly(x_train, degree)\n",
    "x_val_poly = build_poly(x_val, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LS GD(0/49): loss=0.5637145770022857\n",
      "LS GD(1/49): loss=0.5030965588130882\n",
      "LS GD(2/49): loss=0.4677366780732077\n",
      "LS GD(3/49): loss=0.44557016326159926\n",
      "LS GD(4/49): loss=0.4314588150650867\n",
      "LS GD(5/49): loss=0.42265410087896294\n",
      "LS GD(6/49): loss=0.4175400340934408\n",
      "LS GD(7/49): loss=0.4147465905163256\n",
      "LS GD(8/49): loss=0.4133046487913922\n",
      "LS GD(9/49): loss=0.4125344819932907\n",
      "LS GD(10/49): loss=0.4121563853064964\n",
      "LS GD(11/49): loss=0.41197343083728805\n",
      "LS GD(12/49): loss=0.4118857912550087\n",
      "LS GD(13/49): loss=0.4118419041156095\n",
      "LS GD(14/49): loss=0.41181806741002275\n",
      "LS GD(15/49): loss=0.4118036026237632\n",
      "LS GD(16/49): loss=0.411794342846931\n",
      "LS GD(17/49): loss=0.4117886554385048\n",
      "LS GD(18/49): loss=0.41178526805529103\n",
      "LS GD(19/49): loss=0.41172553999938594\n",
      "LS GD(20/49): loss=0.4117181528305998\n",
      "LS GD(21/49): loss=0.41171240562869144\n",
      "LS GD(22/49): loss=0.41170793364034214\n",
      "LS GD(23/49): loss=0.41170445427686025\n",
      "LS GD(24/49): loss=0.41170174766704276\n",
      "LS GD(25/49): loss=0.41169964251533153\n",
      "LS GD(26/49): loss=0.4116980053575932\n",
      "LS GD(27/49): loss=0.41169673223903913\n",
      "LS GD(28/49): loss=0.41169574222350885\n",
      "LS GD(29/49): loss=0.41169497232262764\n",
      "LS GD(30/49): loss=0.41169437353473165\n",
      "LS GD(31/49): loss=0.41169390775247056\n",
      "LS GD(32/49): loss=0.4116935453499819\n",
      "LS GD(33/49): loss=0.4116932633013117\n",
      "LS GD(34/49): loss=0.4116930437138607\n",
      "LS GD(35/49): loss=0.4116928726859493\n",
      "LS GD(36/49): loss=0.4116927394173999\n",
      "LS GD(37/49): loss=0.4116926355175188\n",
      "LS GD(38/49): loss=0.41169255446703523\n",
      "LS GD(39/49): loss=0.41169249119996376\n",
      "LS GD(40/49): loss=0.4116924757040594\n",
      "LS GD(41/49): loss=0.4116924612686368\n",
      "LS GD(42/49): loss=0.41169244782033637\n",
      "LS GD(43/49): loss=0.41169243529090294\n",
      "LS GD(44/49): loss=0.41169242361682895\n",
      "LS GD(45/49): loss=0.41169241273902113\n",
      "LS GD(46/49): loss=0.4116924026024941\n",
      "LS GD(47/49): loss=0.4116923931560849\n",
      "LS GD(48/49): loss=0.4116923843521825\n",
      "LS GD(49/49): loss=0.41169237614648413\n",
      "Train accuracy 0.8168133333333333, Validation Accuracy 0.81596\n"
     ]
    }
   ],
   "source": [
    "# logistic regression GD\n",
    "\n",
    "max_iters = 50\n",
    "gamma = 0.4\n",
    "\n",
    "weights, loss = logistic_regression_SGD(y_train, x_train_poly, initial_w, max_iters, gamma)\n",
    "\n",
    "y_train_pred = predict_labels(weights, x_train_poly)\n",
    "train_acc = cal_acc(y_train, y_train_pred)\n",
    "y_val_pred = predict_labels(weights, x_val_poly)\n",
    "val_acc = cal_acc(y_val, y_val_pred)\n",
    "\n",
    "print(\"Train accuracy {ta}, Validation Accuracy {va}\".format(ta=train_acc, va=val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LS GD(0/49): loss=0.5637408678762021\n",
      "LS GD(1/49): loss=0.5031593129110996\n",
      "LS GD(2/49): loss=0.46780992600747606\n",
      "LS GD(3/49): loss=0.4456515923361605\n",
      "LS GD(4/49): loss=0.4315621302996891\n",
      "LS GD(5/49): loss=0.4227327409874845\n",
      "LS GD(6/49): loss=0.4176010703593492\n",
      "LS GD(7/49): loss=0.414792983094232\n",
      "LS GD(8/49): loss=0.4133408163784908\n",
      "LS GD(9/49): loss=0.4125426547637059\n",
      "LS GD(10/49): loss=0.4121570890798813\n",
      "LS GD(11/49): loss=0.4119744030250716\n",
      "LS GD(12/49): loss=0.4118846006695448\n",
      "LS GD(13/49): loss=0.4118379782486016\n",
      "LS GD(14/49): loss=0.411810826604876\n",
      "LS GD(15/49): loss=0.41179373062995495\n",
      "LS GD(16/49): loss=0.41178389971142854\n",
      "LS GD(17/49): loss=0.41177839015488554\n",
      "LS GD(18/49): loss=0.41177522335212\n",
      "LS GD(19/49): loss=0.411722103946312\n",
      "LS GD(20/49): loss=0.41171620204728027\n",
      "LS GD(21/49): loss=0.41171170019916237\n",
      "LS GD(22/49): loss=0.4117082343623814\n",
      "LS GD(23/49): loss=0.4117055440151946\n",
      "LS GD(24/49): loss=0.4117034406269035\n",
      "LS GD(25/49): loss=0.4117017865652312\n",
      "LS GD(26/49): loss=0.4117004804145949\n",
      "LS GD(27/49): loss=0.41169944651205953\n",
      "LS GD(28/49): loss=0.411698627502149\n",
      "LS GD(29/49): loss=0.4116979791648303\n",
      "LS GD(30/49): loss=0.4116974669080241\n",
      "LS GD(31/49): loss=0.41169706338550127\n",
      "LS GD(32/49): loss=0.4116967468257343\n",
      "LS GD(33/49): loss=0.41169649980729417\n",
      "LS GD(34/49): loss=0.4116963083317983\n",
      "LS GD(35/49): loss=0.4116961611111661\n",
      "LS GD(36/49): loss=0.4116960490173227\n",
      "LS GD(37/49): loss=0.4116959646572913\n",
      "LS GD(38/49): loss=0.41169590204462014\n",
      "LS GD(39/49): loss=0.4116958563435448\n",
      "LS GD(40/49): loss=0.4116958457856478\n",
      "LS GD(41/49): loss=0.41169583622070877\n",
      "LS GD(42/49): loss=0.41169582757340295\n",
      "LS GD(43/49): loss=0.4116958197733411\n",
      "LS GD(44/49): loss=0.4116958127547991\n",
      "LS GD(45/49): loss=0.41169580645645787\n",
      "LS GD(46/49): loss=0.41169580082115487\n",
      "LS GD(47/49): loss=0.41169579579564775\n",
      "LS GD(48/49): loss=0.4116957913303861\n",
      "LS GD(49/49): loss=0.41169578737929713\n",
      "0.816624\n"
     ]
    }
   ],
   "source": [
    "x_poly = build_poly(tX, degree)\n",
    "weights, loss = logistic_regression_SGD(y, x_poly, initial_w, max_iters, gamma)\n",
    "\n",
    "pred = predict_labels(weights, x_poly)\n",
    "acc = cal_acc(y, pred)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda 1e-05, Train accuracy 0.8151422222222222, Validation Accuracy 0.81436\n",
      "Lambda 1.7433288221999873e-05, Train accuracy 0.8151422222222222, Validation Accuracy 0.81436\n",
      "Lambda 3.039195382313195e-05, Train accuracy 0.8151422222222222, Validation Accuracy 0.81432\n",
      "Lambda 5.2983169062837125e-05, Train accuracy 0.8151511111111112, Validation Accuracy 0.81432\n",
      "Lambda 9.236708571873866e-05, Train accuracy 0.8151555555555555, Validation Accuracy 0.81432\n",
      "Lambda 0.00016102620275609394, Train accuracy 0.8151511111111112, Validation Accuracy 0.81436\n",
      "Lambda 0.0002807216203941176, Train accuracy 0.8151377777777777, Validation Accuracy 0.81436\n",
      "Lambda 0.0004893900918477494, Train accuracy 0.8151244444444444, Validation Accuracy 0.81436\n",
      "Lambda 0.0008531678524172815, Train accuracy 0.8151333333333334, Validation Accuracy 0.81436\n",
      "Lambda 0.0014873521072935117, Train accuracy 0.8151288888888889, Validation Accuracy 0.81436\n",
      "Lambda 0.002592943797404667, Train accuracy 0.81512, Validation Accuracy 0.81436\n",
      "Lambda 0.004520353656360245, Train accuracy 0.8151377777777777, Validation Accuracy 0.81436\n",
      "Lambda 0.007880462815669913, Train accuracy 0.8151288888888889, Validation Accuracy 0.81436\n",
      "Lambda 0.013738237958832637, Train accuracy 0.8151377777777777, Validation Accuracy 0.81432\n",
      "Lambda 0.02395026619987486, Train accuracy 0.8151244444444444, Validation Accuracy 0.81432\n",
      "Lambda 0.04175318936560404, Train accuracy 0.8151288888888889, Validation Accuracy 0.81432\n",
      "Lambda 0.07278953843983153, Train accuracy 0.8151377777777777, Validation Accuracy 0.81424\n",
      "Lambda 0.12689610031679235, Train accuracy 0.8143288888888889, Validation Accuracy 0.81328\n",
      "Lambda 0.22122162910704501, Train accuracy 0.79564, Validation Accuracy 0.79504\n",
      "Lambda 0.38566204211634725, Train accuracy 0.81476, Validation Accuracy 0.81412\n",
      "Lambda 0.6723357536499335, Train accuracy 0.7934444444444444, Validation Accuracy 0.79268\n",
      "Lambda 1.1721022975334818, Train accuracy 0.8122844444444445, Validation Accuracy 0.81116\n",
      "Lambda 2.043359717856944, Train accuracy 0.8158177777777778, Validation Accuracy 0.81512\n",
      "Lambda 3.562247890262444, Train accuracy 0.8108577777777778, Validation Accuracy 0.8106\n",
      "Lambda 6.2101694189156165, Train accuracy 0.8147555555555556, Validation Accuracy 0.81412\n",
      "Lambda 10.82636733874054, Train accuracy 0.8085022222222222, Validation Accuracy 0.80768\n",
      "Lambda 18.873918221350994, Train accuracy 0.7067777777777777, Validation Accuracy 0.70204\n",
      "Lambda 32.90344562312671, Train accuracy 0.8086755555555556, Validation Accuracy 0.80796\n",
      "Lambda 57.361525104486816, Train accuracy 0.7920533333333334, Validation Accuracy 0.7918\n",
      "Lambda 100.0, Train accuracy 0.8045288888888888, Validation Accuracy 0.8042\n"
     ]
    }
   ],
   "source": [
    "# ridge regression\n",
    "\n",
    "\n",
    "lambdas = np.logspace(-5, 2, 30)\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    weights, loss = ridge_regression(y_train, x_train_poly, lambda_)\n",
    "\n",
    "    y_train_pred = predict_labels(weights, x_train_poly)\n",
    "    train_acc = cal_acc(y_train, y_train_pred)\n",
    "    y_val_pred = predict_labels(weights, x_val_poly)\n",
    "    val_acc = cal_acc(y_val, y_val_pred)\n",
    "\n",
    "    print(\"Lambda {labd}, Train accuracy {ta}, Validation Accuracy {va}\".format(labd=lambda_, ta=train_acc, va=val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816132\n"
     ]
    }
   ],
   "source": [
    "# best para of ridge regression\n",
    "\n",
    "ratio = 0.9\n",
    "degree = 10\n",
    "lambda_ = 9.236708571873866e-05\n",
    "seed = 6\n",
    "\n",
    "x_poly = build_poly(tX, degree)\n",
    "weights, loss = ridge_regression(y, x_poly, lambda_)\n",
    "\n",
    "pred = predict_labels(weights, x_poly)\n",
    "acc = cal_acc(y, pred)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "# pre-process\n",
    "bad_data = np.where(tX_test==-999)\n",
    "bad_col_data = np.where(tX_test[:, -1]==0)\n",
    "\n",
    "tX_test = (tX_test-col_avg) / col_std\n",
    "tX_test[bad_data] = 0\n",
    "tX_test[:, -1][bad_col_data] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'predictions.csv' # TODO: fill in desired name of output file for submission\n",
    "# y_pred = predict_labels(weights, tX_test)\n",
    "# for polynomial fit\n",
    "x_test_poly = build_poly(tX_test, degree)\n",
    "y_pred = predict_labels(weights, x_test_poly)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "gc_course_env",
   "language": "python",
   "name": "gc_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
