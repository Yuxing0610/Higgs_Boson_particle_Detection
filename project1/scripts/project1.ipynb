{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing: normalize each feature, then fill bad data with average (0)\n",
    "\n",
    "num_feature = tX.shape[1]\n",
    "col_avg = np.zeros(num_feature)\n",
    "col_std = np.zeros(num_feature)\n",
    "\n",
    "# calculate the average and stadard deviation of each data coloum\n",
    "for i in range(num_feature):\n",
    "    cur_col = tX[:, i]\n",
    "    if i == num_feature-1:\n",
    "        good_data = np.where(cur_col!=0)\n",
    "    else:\n",
    "        good_data = np.where(cur_col!=-999)\n",
    "    col_avg[i] = np.mean(cur_col[good_data])\n",
    "    col_std[i] = np.std(cur_col[good_data])\n",
    "    \n",
    "col_avg = col_avg[np.newaxis, :]\n",
    "col_std = col_std[np.newaxis, :]\n",
    "\n",
    "# pick bad data\n",
    "bad_data = np.where(tX==-999)\n",
    "bad_col_data = np.where(tX[:, -1]==0)\n",
    "\n",
    "# normalization\n",
    "tX = (tX-col_avg) / col_std\n",
    "\n",
    "# filling bad data with average\n",
    "tX[bad_data] = 0\n",
    "tX[:, -1][bad_col_data] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required implementations\n",
    "\n",
    "# max_iters = 50\n",
    "# gamma = 0.1 # least square GD & logistic_regression & reg_logistic_regression\n",
    "# gamma = 0.001 # least square SGD\n",
    "# lambda_ = 0.1\n",
    "# initial_w = np.zeros(tX.shape[1])\n",
    "\n",
    "# weights, loss = least_squares_GD(y, tX, initial_w, max_iters, gamma)\n",
    "# weights, loss = least_squares_SGD(y, tX, initial_w, max_iters, gamma)\n",
    "# weights, loss = least_squares(y, tX)\n",
    "# weights, loss = ridge_regression(y, tX, lambda_)\n",
    "# weights, loss = logistic_regression(y, tX, initial_w, max_iters, gamma)\n",
    "# weights, loss = reg_logistic_regression(y, tX, lambda_, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda 1e-05, Train accuracy 0.8161539999999998, Validation Accuracy 0.8157080000000001\n",
      "Lambda 2.3357214690901213e-05, Train accuracy 0.816149, Validation Accuracy 0.8157080000000001\n",
      "Lambda 5.4555947811685143e-05, Train accuracy 0.8161400000000001, Validation Accuracy 0.8157119999999999\n",
      "Lambda 0.00012742749857031334, Train accuracy 0.816137, Validation Accuracy 0.8156960000000002\n",
      "Lambda 0.00029763514416313193, Train accuracy 0.8161400000000001, Validation Accuracy 0.8157039999999999\n",
      "Lambda 0.0006951927961775605, Train accuracy 0.8161390000000001, Validation Accuracy 0.8156880000000001\n",
      "Lambda 0.001623776739188721, Train accuracy 0.8161400000000001, Validation Accuracy 0.8156880000000001\n",
      "Lambda 0.00379269019073225, Train accuracy 0.816136, Validation Accuracy 0.815704\n",
      "Lambda 0.008858667904100823, Train accuracy 0.81616, Validation Accuracy 0.81572\n",
      "Lambda 0.02069138081114788, Train accuracy 0.816151, Validation Accuracy 0.815736\n",
      "Lambda 0.04832930238571752, Train accuracy 0.816154, Validation Accuracy 0.815724\n",
      "Lambda 0.11288378916846883, Train accuracy 0.8161539999999998, Validation Accuracy 0.815768\n",
      "Lambda 0.26366508987303555, Train accuracy 0.8161970000000001, Validation Accuracy 0.815728\n",
      "Lambda 0.6158482110660255, Train accuracy 0.8162220000000001, Validation Accuracy 0.8157400000000001\n",
      "Lambda 1.438449888287663, Train accuracy 0.81616, Validation Accuracy 0.815692\n",
      "Lambda 3.359818286283781, Train accuracy 0.815741, Validation Accuracy 0.815192\n",
      "Lambda 7.847599703514606, Train accuracy 0.816066, Validation Accuracy 0.8156359999999999\n",
      "Lambda 18.32980710832434, Train accuracy 0.816192, Validation Accuracy 0.815832\n",
      "Lambda 42.81332398719387, Train accuracy 0.815865, Validation Accuracy 0.8154199999999999\n",
      "Lambda 100.0, Train accuracy 0.815898, Validation Accuracy 0.815452\n"
     ]
    }
   ],
   "source": [
    "# polynomial data augmentation\n",
    "\n",
    "degree = 10\n",
    "\n",
    "initial_w = np.zeros(tX.shape[1]*degree)\n",
    "x_poly = build_poly(tX, degree)\n",
    "\n",
    "# ridge regression\n",
    "\n",
    "K = 5\n",
    "seed = 6\n",
    "\n",
    "lambdas = np.logspace(-5, 2, 20)\n",
    "for lambda_ in lambdas:\n",
    "    \n",
    "    # cross validation\n",
    "    train_acc, val_acc = cross_validation(y, x_poly, K, seed, ridge_regression, lambda_)\n",
    "\n",
    "    print(\"Lambda {labd}, Train accuracy {ta}, Validation Accuracy {va}\".format(labd=lambda_, ta=np.mean(train_acc), va=np.mean(val_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816132\n"
     ]
    }
   ],
   "source": [
    "# best para of ridge regression\n",
    "\n",
    "ratio = 0.9\n",
    "degree = 10\n",
    "lambda_ = 9.236708571873866e-05\n",
    "seed = 6\n",
    "\n",
    "x_poly = build_poly(tX, degree)\n",
    "weights, loss = ridge_regression(y, x_poly, lambda_)\n",
    "\n",
    "pred = predict_labels(weights, x_poly)\n",
    "acc = cal_acc(y, pred)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LS GD(0/49): loss=0.5637145770022863\n",
      "LS GD(1/49): loss=0.5030965871924096\n",
      "LS GD(2/49): loss=0.46773667027252425\n",
      "LS GD(3/49): loss=0.44557017340963617\n",
      "LS GD(4/49): loss=0.4314588286880356\n",
      "LS GD(5/49): loss=0.42265411088294524\n",
      "LS GD(6/49): loss=0.41754003926894434\n",
      "LS GD(7/49): loss=0.41474659195900354\n",
      "LS GD(8/49): loss=0.4133046495999577\n",
      "LS GD(9/49): loss=0.4125344821257037\n",
      "LS GD(10/49): loss=0.4121563852803343\n",
      "LS GD(11/49): loss=0.41197343078576665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xingyu/Desktop/EPFL_semester1/ml-project-1-ssy_mlproject1/project1/scripts/implementations.py:64: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LS GD(12/49): loss=0.4118857912121183\n",
      "LS GD(13/49): loss=0.41184190408260984\n",
      "LS GD(14/49): loss=0.41181806738449883\n",
      "LS GD(15/49): loss=0.4118036026042454\n",
      "LS GD(16/49): loss=0.41179434283349325\n",
      "LS GD(17/49): loss=0.41178865543011756\n",
      "LS GD(18/49): loss=0.4117852680502254\n",
      "LS GD(19/49): loss=0.4117255400056201\n",
      "LS GD(20/49): loss=0.4117181528356375\n",
      "LS GD(21/49): loss=0.4117124056328053\n",
      "LS GD(22/49): loss=0.41170793364373964\n",
      "LS GD(23/49): loss=0.41170445427969854\n",
      "LS GD(24/49): loss=0.41170174766943873\n",
      "LS GD(25/49): loss=0.41169964251737484\n",
      "LS GD(26/49): loss=0.4116980053593512\n",
      "LS GD(27/49): loss=0.41169673224056175\n",
      "LS GD(28/49): loss=0.41169574222483696\n",
      "LS GD(29/49): loss=0.41169497232379115\n",
      "LS GD(30/49): loss=0.4116943735357542\n",
      "LS GD(31/49): loss=0.4116939077533708\n",
      "LS GD(32/49): loss=0.41169354535077685\n",
      "LS GD(33/49): loss=0.4116932633020138\n",
      "LS GD(34/49): loss=0.4116930437144813\n",
      "LS GD(35/49): loss=0.4116928726864973\n",
      "LS GD(36/49): loss=0.41169273941788265\n",
      "LS GD(37/49): loss=0.41169263551794444\n",
      "LS GD(38/49): loss=0.41169255446740943\n",
      "LS GD(39/49): loss=0.4116924912002927\n",
      "LS GD(40/49): loss=0.41169247570437545\n",
      "LS GD(41/49): loss=0.41169246126894193\n",
      "LS GD(42/49): loss=0.4116924478206288\n",
      "LS GD(43/49): loss=0.41169243529118466\n",
      "LS GD(44/49): loss=0.41169242361709946\n",
      "LS GD(45/49): loss=0.41169241273928137\n",
      "LS GD(46/49): loss=0.4116924026027442\n",
      "LS GD(47/49): loss=0.41169239315632555\n",
      "LS GD(48/49): loss=0.4116923843524144\n",
      "LS GD(49/49): loss=0.4116923761467068\n",
      "Train accuracy 0.8168133333333333, Validation Accuracy 0.81596\n"
     ]
    }
   ],
   "source": [
    "# polynomial data augmentation\n",
    "\n",
    "degree = 4\n",
    "\n",
    "initial_w = np.zeros(tX.shape[1]*degree)\n",
    "x_train, y_train, x_val, y_val = split_data(tX, y, ratio, seed)\n",
    "x_train_poly = build_poly(x_train, degree)\n",
    "x_val_poly = build_poly(x_val, degree)\n",
    "\n",
    "# best para of logistic regression Newton\n",
    "\n",
    "max_iters = 50\n",
    "gamma = 0.4\n",
    "\n",
    "weights, loss = logistic_regression_newton(y_train, x_train_poly, initial_w, max_iters, gamma)\n",
    "\n",
    "y_train_pred = predict_labels(weights, x_train_poly)\n",
    "train_acc = cal_acc(y_train, y_train_pred)\n",
    "y_val_pred = predict_labels(weights, x_val_poly)\n",
    "val_acc = cal_acc(y_val, y_val_pred)\n",
    "\n",
    "print(\"Train accuracy {ta}, Validation Accuracy {va}\".format(ta=train_acc, va=val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "# pre-process\n",
    "bad_data = np.where(tX_test==-999)\n",
    "bad_col_data = np.where(tX_test[:, -1]==0)\n",
    "\n",
    "tX_test = (tX_test-col_avg) / col_std\n",
    "tX_test[bad_data] = 0\n",
    "tX_test[:, -1][bad_col_data] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'predictions.csv' # TODO: fill in desired name of output file for submission\n",
    "\n",
    "# polynomial data augmentation\n",
    "x_test_poly = build_poly(tX_test, degree)\n",
    "y_pred = predict_labels(weights, x_test_poly)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "gc_course_env",
   "language": "python",
   "name": "gc_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
