{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing: normalize each feature, then fill bad data with average (0)\n",
    "\n",
    "num_feature = tX.shape[1]\n",
    "col_avg = np.zeros(num_feature)\n",
    "col_std = np.zeros(num_feature)\n",
    "\n",
    "# calculate the average and stadard deviation of each data coloum\n",
    "for i in range(num_feature):\n",
    "    cur_col = tX[:, i]\n",
    "    if i == num_feature-1:\n",
    "        good_data = np.where(cur_col!=0)\n",
    "    else:\n",
    "        good_data = np.where(cur_col!=-999)\n",
    "    col_avg[i] = np.mean(cur_col[good_data])\n",
    "    col_std[i] = np.std(cur_col[good_data])\n",
    "    \n",
    "col_avg = col_avg[np.newaxis, :]\n",
    "col_std = col_std[np.newaxis, :]\n",
    "\n",
    "# pick bad data\n",
    "bad_data = np.where(tX==-999)\n",
    "bad_col_data = np.where(tX[:, -1]==0)\n",
    "\n",
    "# normalization\n",
    "tX = (tX-col_avg) / col_std\n",
    "\n",
    "# filling bad data with average\n",
    "tX[bad_data] = 0\n",
    "tX[:, -1][bad_col_data] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required implementations\n",
    "\n",
    "# max_iters = 50\n",
    "# gamma = 0.1 # least square GD & logistic_regression & reg_logistic_regression\n",
    "# gamma = 0.001 # least square SGD\n",
    "# lambda_ = 0.1\n",
    "# initial_w = np.zeros(tX.shape[1])\n",
    "\n",
    "# weights, loss = least_squares_GD(y, tX, initial_w, max_iters, gamma)\n",
    "# weights, loss = least_squares_SGD(y, tX, initial_w, max_iters, gamma)\n",
    "# weights, loss = least_squares(y, tX)\n",
    "# weights, loss = ridge_regression(y, tX, lambda_)\n",
    "# weights, loss = logistic_regression(y, tX, initial_w, max_iters, gamma)\n",
    "# weights, loss = reg_logistic_regression(y, tX, lambda_, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial data augmentation\n",
    "\n",
    "degree = 10\n",
    "\n",
    "initial_w = np.zeros(tX.shape[1]*degree)\n",
    "x_ploy = build_poly(tX, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda 1e-05, Train accuracy 0.8161539999999998, Validation Accuracy 0.8157080000000001\n",
      "Lambda 1.7433288221999873e-05, Train accuracy 0.816143, Validation Accuracy 0.8157080000000001\n",
      "Lambda 3.039195382313195e-05, Train accuracy 0.816146, Validation Accuracy 0.8157080000000001\n",
      "Lambda 5.2983169062837125e-05, Train accuracy 0.816141, Validation Accuracy 0.8157119999999999\n",
      "Lambda 9.236708571873866e-05, Train accuracy 0.816139, Validation Accuracy 0.8157119999999999\n",
      "Lambda 0.00016102620275609394, Train accuracy 0.816139, Validation Accuracy 0.815692\n",
      "Lambda 0.0002807216203941176, Train accuracy 0.8161400000000001, Validation Accuracy 0.8156960000000002\n",
      "Lambda 0.0004893900918477494, Train accuracy 0.8161400000000001, Validation Accuracy 0.8157\n",
      "Lambda 0.0008531678524172815, Train accuracy 0.8161419999999999, Validation Accuracy 0.815692\n",
      "Lambda 0.0014873521072935117, Train accuracy 0.816141, Validation Accuracy 0.815692\n",
      "Lambda 0.002592943797404667, Train accuracy 0.816141, Validation Accuracy 0.815692\n",
      "Lambda 0.004520353656360245, Train accuracy 0.816141, Validation Accuracy 0.8157119999999999\n",
      "Lambda 0.007880462815669913, Train accuracy 0.8161609999999999, Validation Accuracy 0.81572\n",
      "Lambda 0.013738237958832637, Train accuracy 0.816154, Validation Accuracy 0.815736\n",
      "Lambda 0.02395026619987486, Train accuracy 0.816143, Validation Accuracy 0.8157439999999999\n",
      "Lambda 0.04175318936560404, Train accuracy 0.816158, Validation Accuracy 0.815732\n",
      "Lambda 0.07278953843983153, Train accuracy 0.816149, Validation Accuracy 0.815756\n",
      "Lambda 0.12689610031679235, Train accuracy 0.816147, Validation Accuracy 0.815768\n",
      "Lambda 0.22122162910704501, Train accuracy 0.8161860000000001, Validation Accuracy 0.81576\n",
      "Lambda 0.38566204211634725, Train accuracy 0.8161660000000002, Validation Accuracy 0.815752\n",
      "Lambda 0.6723357536499335, Train accuracy 0.8161950000000001, Validation Accuracy 0.815724\n",
      "Lambda 1.1721022975334818, Train accuracy 0.8160910000000001, Validation Accuracy 0.8156120000000001\n",
      "Lambda 2.043359717856944, Train accuracy 0.8162609999999999, Validation Accuracy 0.815804\n",
      "Lambda 3.562247890262444, Train accuracy 0.816262, Validation Accuracy 0.815808\n",
      "Lambda 6.2101694189156165, Train accuracy 0.816132, Validation Accuracy 0.815628\n",
      "Lambda 10.82636733874054, Train accuracy 0.8080359999999999, Validation Accuracy 0.8075319999999999\n",
      "Lambda 18.873918221350994, Train accuracy 0.815917, Validation Accuracy 0.81554\n",
      "Lambda 32.90344562312671, Train accuracy 0.8159799999999999, Validation Accuracy 0.8155759999999999\n",
      "Lambda 57.361525104486816, Train accuracy 0.8161909999999999, Validation Accuracy 0.815832\n",
      "Lambda 100.0, Train accuracy 0.815898, Validation Accuracy 0.815452\n"
     ]
    }
   ],
   "source": [
    "# ridge regression\n",
    "\n",
    "K = 5\n",
    "seed = 6\n",
    "\n",
    "lambdas = np.logspace(-5, 2, 30)\n",
    "for lambda_ in lambdas:\n",
    "    \n",
    "    # cross validation\n",
    "    train_acc, val_acc = cross_validation(y, x_poly, K, seed, ridge_regression, lambda_)\n",
    "\n",
    "    print(\"Lambda {labd}, Train accuracy {ta}, Validation Accuracy {va}\".format(labd=lambda_, ta=np.mean(train_acc), va=np.mean(val_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression: loss=0.552328297165218\n",
      "0.816132\n"
     ]
    }
   ],
   "source": [
    "# best para of ridge regression\n",
    "\n",
    "ratio = 0.9\n",
    "degree = 10\n",
    "lambda_ = 9.236708571873866e-05\n",
    "seed = 6\n",
    "\n",
    "x_poly = build_poly(tX, degree)\n",
    "weights, loss = ridge_regression(y, x_poly, lambda_)\n",
    "\n",
    "pred = predict_labels(weights, x_poly)\n",
    "acc = cal_acc(y, pred)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "# pre-process\n",
    "bad_data = np.where(tX_test==-999)\n",
    "bad_col_data = np.where(tX_test[:, -1]==0)\n",
    "\n",
    "tX_test = (tX_test-col_avg) / col_std\n",
    "tX_test[bad_data] = 0\n",
    "tX_test[:, -1][bad_col_data] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'predictions.csv' # TODO: fill in desired name of output file for submission\n",
    "\n",
    "# polynomial data augmentation\n",
    "x_test_poly = build_poly(tX_test, degree)\n",
    "y_pred = predict_labels(weights, x_test_poly)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "gc_course_env",
   "language": "python",
   "name": "gc_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
